#!/usr/bin/env python3
"""
Memorin Redisç¼“å­˜ç®¡ç†å·¥å…·
æä¾›ç¼“å­˜é¢„çƒ­ã€æ¸…ç†ã€ç›‘æ§ã€ä¸€è‡´æ€§æ£€æŸ¥ç­‰åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
    python redis_cache_management.py --warmup  # ç¼“å­˜é¢„çƒ­
    python redis_cache_management.py --cleanup # ç¼“å­˜æ¸…ç†
    python redis_cache_management.py --monitor # ç›‘æ§æŒ‡æ ‡
    python redis_cache_management.py --check   # ä¸€è‡´æ€§æ£€æŸ¥
"""

import redis
import mysql.connector
import json
import gzip
import base64
import time
import random
import argparse
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from redis.connection import ConnectionPool
import sys
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/redis/cache_management.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RedisCacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.config = config
        self.redis_master = None
        self.redis_slave = None
        self.db_connection = None
        self.hit_count = 0
        self.miss_count = 0
        self.total_requests = 0
        
        self._init_redis_connections()
        self._init_db_connection()
    
    def _init_redis_connections(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            # ä¸»Redisè¿æ¥æ± 
            master_pool = ConnectionPool(
                host=self.config['redis']['master']['host'],
                port=self.config['redis']['master']['port'],
                password=self.config['redis']['master'].get('password'),
                db=0,
                max_connections=100,
                retry_on_timeout=True,
                socket_timeout=5,
                socket_connect_timeout=5,
                health_check_interval=30
            )
            self.redis_master = redis.Redis(connection_pool=master_pool)
            
            # ä»Redisè¿æ¥æ± 
            if 'slave' in self.config['redis']:
                slave_pool = ConnectionPool(
                    host=self.config['redis']['slave']['host'],
                    port=self.config['redis']['slave']['port'],
                    password=self.config['redis']['slave'].get('password'),
                    db=0,
                    max_connections=50,
                    retry_on_timeout=True,
                    socket_timeout=5,
                    socket_connect_timeout=5,
                    health_check_interval=30
                )
                self.redis_slave = redis.Redis(connection_pool=slave_pool)
            else:
                self.redis_slave = self.redis_master
            
            logger.info("Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Redisè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def _init_db_connection(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            self.db_connection = mysql.connector.connect(
                host=self.config['database']['host'],
                port=self.config['database']['port'],
                user=self.config['database']['user'],
                password=self.config['database']['password'],
                database=self.config['database']['name'],
                autocommit=True,
                charset='utf8mb4'
            )
            logger.info("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def warmup_cache(self):
        """ç³»ç»Ÿå¯åŠ¨æ—¶çš„ç¼“å­˜é¢„çƒ­"""
        logger.info("å¼€å§‹ç¼“å­˜é¢„çƒ­...")
        
        try:
            # 1. é¢„çƒ­çƒ­ç‚¹çŸ¥è¯†åº“
            self._warmup_popular_knowledge_bases()
            
            # 2. é¢„çƒ­ç³»ç»Ÿé…ç½®
            self._warmup_system_configs()
            
            # 3. é¢„çƒ­å…¨å±€ç»Ÿè®¡
            self._warmup_global_statistics()
            
            # 4. é¢„çƒ­ç”¨æˆ·ä¼šè¯ï¼ˆæœ€è¿‘æ´»è·ƒç”¨æˆ·ï¼‰
            self._warmup_active_user_sessions()
            
            logger.info("ç¼“å­˜é¢„çƒ­å®Œæˆ!")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")
            raise
    
    def _warmup_popular_knowledge_bases(self):
        """é¢„çƒ­çƒ­ç‚¹çŸ¥è¯†åº“"""
        logger.info("é¢„çƒ­çƒ­ç‚¹çŸ¥è¯†åº“...")
        
        cursor = self.db_connection.cursor(dictionary=True)
        
        # è·å–çƒ­é—¨çŸ¥è¯†åº“
        cursor.execute("""
            SELECT id, name, owner_id, visibility, description, icon, difficulty_level,
                   content_count, subscriber_count, created_at, updated_at
            FROM knowledge_bases 
            WHERE visibility = 'public' 
            ORDER BY subscriber_count DESC 
            LIMIT %s
        """, [self.config['cache']['warmup_popular_kb_count']])
        
        popular_kbs = cursor.fetchall()
        
        for kb in popular_kbs:
            self._preload_knowledge_base(kb['id'], kb)
        
        cursor.close()
        logger.info(f"é¢„çƒ­äº† {len(popular_kbs)} ä¸ªçƒ­é—¨çŸ¥è¯†åº“")
    
    def _preload_knowledge_base(self, kb_id: int, kb_info: Dict[str, Any]):
        """é¢„åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
        try:
            # 1. ç¼“å­˜åŸºæœ¬ä¿¡æ¯
            kb_cache_data = {
                'id': str(kb_info['id']),
                'name': kb_info['name'],
                'owner_id': str(kb_info['owner_id']),
                'visibility': kb_info['visibility'],
                'content_count': str(kb_info['content_count']),
                'subscriber_count': str(kb_info['subscriber_count']),
                'created_at': kb_info['created_at'].isoformat() if kb_info['created_at'] else '',
                'updated_at': kb_info['updated_at'].isoformat() if kb_info['updated_at'] else ''
            }
            
            self.redis_master.hset(f"kb:info:{kb_id}", mapping=kb_cache_data)
            self.redis_master.expire(f"kb:info:{kb_id}", 604800)  # 7å¤©
            
            # 2. ç¼“å­˜è¯¦ç»†ä¿¡æ¯
            kb_detail = {
                'metadata': {
                    'description': kb_info['description'] or '',
                    'icon': kb_info['icon'] or 'ğŸ“š',
                    'difficulty_level': float(kb_info['difficulty_level'] or 0),
                    'tags': []
                },
                'areas': self._get_knowledge_areas(kb_id),
                'statistics': self._get_kb_statistics(kb_id)
            }
            
            compressed_detail = self._compress_large_object(kb_detail)
            self.redis_master.setex(f"kb:detail:{kb_id}", 604800, compressed_detail)
            
            # 3. é¢„åŠ è½½çƒ­ç‚¹çŸ¥è¯†ç‚¹
            self._preload_hot_knowledge_points(kb_id)
            
        except Exception as e:
            logger.error(f"é¢„åŠ è½½çŸ¥è¯†åº“ {kb_id} å¤±è´¥: {e}")
    
    def _get_knowledge_areas(self, kb_id: int) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†åº“çš„çŸ¥è¯†åŒºåŸŸ"""
        cursor = self.db_connection.cursor(dictionary=True)
        cursor.execute("""
            SELECT id, name, description, parent_id, level, sort_order
            FROM knowledge_areas 
            WHERE knowledge_base_id = %s 
            ORDER BY level, sort_order
        """, [kb_id])
        
        areas = cursor.fetchall()
        cursor.close()
        
        return [
            {
                'id': area['id'],
                'name': area['name'],
                'description': area['description'] or '',
                'parent_id': area['parent_id'],
                'level': area['level'],
                'sort_order': area['sort_order']
            }
            for area in areas
        ]
    
    def _get_kb_statistics(self, kb_id: int) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        cursor = self.db_connection.cursor(dictionary=True)
        
        # åŸºç¡€ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                COUNT(*) as total_contents,
                AVG(difficulty_level) as avg_difficulty
            FROM knowledge_point_contents kpc
            JOIN knowledge_base_content_relations kbcr ON kpc.id = kbcr.content_id
            WHERE kbcr.knowledge_base_id = %s
        """, [kb_id])
        
        stats = cursor.fetchone()
        cursor.close()
        
        return {
            'total_contents': stats['total_contents'] or 0,
            'avg_difficulty': float(stats['avg_difficulty'] or 0),
            'completion_rate': 0.0,  # éœ€è¦ç”¨æˆ·ç™»å½•åè®¡ç®—
            'last_updated': datetime.now().isoformat()
        }
    
    def _preload_hot_knowledge_points(self, kb_id: int):
        """é¢„åŠ è½½çƒ­ç‚¹çŸ¥è¯†ç‚¹"""
        cursor = self.db_connection.cursor(dictionary=True)
        
        # è·å–çƒ­ç‚¹å†…å®¹
        cursor.execute("""
            SELECT kpc.id, kpc.question, kpc.answer, kpc.type, kpc.difficulty_level, 
                   kpc.explanation, kbcr.usage_count
            FROM knowledge_point_contents kpc
            JOIN knowledge_base_content_relations kbcr ON kpc.id = kbcr.content_id
            WHERE kbcr.knowledge_base_id = %s 
            ORDER BY kbcr.usage_count DESC 
            LIMIT %s
        """, [kb_id, self.config['cache']['warmup_hot_content_count']])
        
        hot_contents = cursor.fetchall()
        
        for content in hot_contents:
            self._preload_knowledge_point(content)
        
        cursor.close()
        logger.info(f"ä¸ºçŸ¥è¯†åº“ {kb_id} é¢„åŠ è½½äº† {len(hot_contents)} ä¸ªçƒ­ç‚¹çŸ¥è¯†ç‚¹")
    
    def _preload_knowledge_point(self, content: Dict[str, Any]):
        """é¢„åŠ è½½çŸ¥è¯†ç‚¹å†…å®¹"""
        content_id = content['id']
        
        # ç¼“å­˜å†…å®¹æ•°æ®
        content_cache_data = {
            'question': content['question'] or '',
            'answer': content['answer'] or '',
            'type': content['type'] or 'fill',
            'difficulty': str(content['difficulty_level'] or 1),
            'explanation': content['explanation'] or ''
        }
        
        self.redis_master.hset(f"kp:content:{content_id}", mapping=content_cache_data)
        self.redis_master.expire(f"kp:content:{content_id}", 604800)  # 7å¤©
        
        # å¦‚æœæ˜¯é€‰æ‹©é¢˜ï¼ŒåŠ è½½é€‰é¡¹
        if content['type'] == 'choice':
            choices = self._get_knowledge_point_choices(content_id)
            if choices:
                self.redis_master.setex(
                    f"kp:choices:{content_id}", 
                    604800, 
                    json.dumps(choices, ensure_ascii=False)
                )
    
    def _get_knowledge_point_choices(self, content_id: int) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†ç‚¹é€‰æ‹©é¢˜é€‰é¡¹"""
        cursor = self.db_connection.cursor(dictionary=True)
        cursor.execute("""
            SELECT choice_key, choice_text, is_correct, explanation
            FROM knowledge_point_choices 
            WHERE content_id = %s 
            ORDER BY choice_key
        """, [content_id])
        
        choices = cursor.fetchall()
        cursor.close()
        
        return [
            {
                'key': choice['choice_key'],
                'text': choice['choice_text'],
                'is_correct': bool(choice['is_correct']),
                'explanation': choice['explanation'] or ''
            }
            for choice in choices
        ]
    
    def _warmup_system_configs(self):
        """é¢„çƒ­ç³»ç»Ÿé…ç½®"""
        logger.info("é¢„çƒ­ç³»ç»Ÿé…ç½®...")
        
        # ç³»ç»Ÿé…ç½®é€šå¸¸å¾ˆå°‘ï¼Œå¯ä»¥å…¨éƒ¨åŠ è½½
        system_configs = {
            'app_name': 'Memorin',
            'version': '2.0.0',
            'maintenance_mode': False,
            'max_daily_reviews': 200,
            'default_difficulty': 3,
            'review_intervals': [1, 3, 7, 15, 30, 90, 180, 365]
        }
        
        for key, value in system_configs.items():
            self.redis_master.setex(f"config:{key}", 86400, json.dumps(value))
    
    def _warmup_global_statistics(self):
        """é¢„çƒ­å…¨å±€ç»Ÿè®¡"""
        logger.info("é¢„çƒ­å…¨å±€ç»Ÿè®¡...")
        
        cursor = self.db_connection.cursor(dictionary=True)
        today = datetime.now().strftime('%Y-%m-%d')
        
        # å…¨å±€ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                COUNT(DISTINCT user_id) as active_users,
                COUNT(*) as total_reviews,
                AVG(CASE WHEN is_correct THEN 1 ELSE 0 END) as avg_accuracy
            FROM user_review_histories 
            WHERE DATE(reviewed_at) = %s
        """, [today])
        
        global_stats = cursor.fetchone()
        
        stats_data = {
            'total_reviews': str(global_stats['total_reviews'] or 0),
            'total_users_active': str(global_stats['active_users'] or 0),
            'avg_accuracy': f"{float(global_stats['avg_accuracy'] or 0):.2f}",
            'total_new_content': '0'  # éœ€è¦å•ç‹¬è®¡ç®—
        }
        
        self.redis_master.hset(f"stats:global:daily:{today}", mapping=stats_data)
        self.redis_master.expire(f"stats:global:daily:{today}", 86400)
        
        cursor.close()
    
    def _warmup_active_user_sessions(self):
        """é¢„çƒ­æ´»è·ƒç”¨æˆ·ä¼šè¯"""
        logger.info("é¢„çƒ­æ´»è·ƒç”¨æˆ·ä¼šè¯...")
        
        cursor = self.db_connection.cursor(dictionary=True)
        
        # è·å–æœ€è¿‘æ´»è·ƒçš„ç”¨æˆ·
        cursor.execute("""
            SELECT DISTINCT user_id
            FROM user_review_histories 
            WHERE reviewed_at >= %s
            LIMIT 100
        """, [datetime.now() - timedelta(hours=24)])
        
        active_users = cursor.fetchall()
        
        for user in active_users:
            self._preload_user_session_data(user['user_id'])
        
        cursor.close()
        logger.info(f"é¢„çƒ­äº† {len(active_users)} ä¸ªæ´»è·ƒç”¨æˆ·ä¼šè¯")
    
    def _preload_user_session_data(self, user_id: int):
        """é¢„åŠ è½½ç”¨æˆ·ä¼šè¯æ•°æ®"""
        cursor = self.db_connection.cursor(dictionary=True)
        
        # ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        cursor.execute("""
            SELECT id, username, email, status, last_login
            FROM users 
            WHERE id = %s
        """, [user_id])
        
        user_info = cursor.fetchone()
        if user_info:
            user_cache_data = {
                'id': str(user_info['id']),
                'username': user_info['username'],
                'email': user_info['email'],
                'status': user_info['status'],
                'last_login': user_info['last_login'].isoformat() if user_info['last_login'] else ''
            }
            
            self.redis_master.hset(f"user:info:{user_id}", mapping=user_cache_data)
            self.redis_master.expire(f"user:info:{user_id}", 86400)  # 24å°æ—¶
        
        cursor.close()
    
    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸå’Œä½ä»·å€¼ç¼“å­˜"""
        logger.info("å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜...")
        
        try:
            # 1. æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·ä¼šè¯
            self._cleanup_expired_user_sessions()
            
            # 2. æ¸…ç†ä½è®¿é—®é‡çš„å†…å®¹ç¼“å­˜
            self._cleanup_low_value_content_cache()
            
            # 3. æ¸…ç†è¿‡æœŸçš„ç»Ÿè®¡ç¼“å­˜
            self._cleanup_expired_statistics()
            
            # 4. å†…å­˜æ•´ç†
            self.redis_master.execute_command('MEMORY', 'PURGE')
            
            logger.info("ç¼“å­˜æ¸…ç†å®Œæˆ!")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
            raise
    
    def _cleanup_expired_user_sessions(self):
        """æ¸…ç†è¿‡æœŸç”¨æˆ·ä¼šè¯"""
        logger.info("æ¸…ç†è¿‡æœŸç”¨æˆ·ä¼šè¯...")
        
        online_keys = self.redis_slave.keys('user:online:*')
        cleaned_count = 0
        
        for key in online_keys:
            last_activity = self.redis_slave.get(key)
            if last_activity:
                try:
                    last_time = float(last_activity.decode())
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡30åˆ†é’Ÿæœªæ´»åŠ¨
                    if time.time() - last_time > self.config['cache']['cleanup_expired_session_threshold']:
                        user_id = key.decode().split(':')[-1]
                        # æ¸…ç†ç›¸å…³ç¼“å­˜
                        self.redis_master.delete(f"user:online:{user_id}")
                        self.redis_master.delete(f"user:session:{user_id}")
                        cleaned_count += 1
                except (ValueError, UnicodeDecodeError):
                    # æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç›´æ¥åˆ é™¤
                    self.redis_master.delete(key)
                    cleaned_count += 1
        
        logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç”¨æˆ·ä¼šè¯")
    
    def _cleanup_low_value_content_cache(self):
        """æ¸…ç†ä½ä»·å€¼å†…å®¹ç¼“å­˜"""
        logger.info("æ¸…ç†ä½ä»·å€¼å†…å®¹ç¼“å­˜...")
        
        access_keys = self.redis_slave.keys('access:count:kp:*')
        cleaned_count = 0
        
        for key in access_keys:
            count = self.redis_slave.get(key)
            if count:
                try:
                    access_count = int(count.decode())
                    if access_count < self.config['cache']['cleanup_low_access_threshold']:
                        content_id = key.decode().split(':')[-1]
                        # åˆ é™¤å†…å®¹ç¼“å­˜
                        self.redis_master.delete(f"kp:content:{content_id}")
                        self.redis_master.delete(f"kp:choices:{content_id}")
                        cleaned_count += 1
                except (ValueError, UnicodeDecodeError):
                    pass
        
        logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªä½è®¿é—®é‡å†…å®¹ç¼“å­˜")
    
    def _cleanup_expired_statistics(self):
        """æ¸…ç†è¿‡æœŸç»Ÿè®¡ç¼“å­˜"""
        logger.info("æ¸…ç†è¿‡æœŸç»Ÿè®¡ç¼“å­˜...")
        
        # æ¸…ç†7å¤©å‰çš„æ—¥ç»Ÿè®¡æ•°æ®
        cutoff_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        
        # æ¸…ç†å…¨å±€ç»Ÿè®¡
        global_stats_keys = self.redis_slave.keys('stats:global:daily:*')
        cleaned_count = 0
        
        for key in global_stats_keys:
            key_str = key.decode()
            date_str = key_str.split(':')[-1]
            if date_str < cutoff_date:
                self.redis_master.delete(key)
                cleaned_count += 1
        
        # æ¸…ç†çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats_keys = self.redis_slave.keys('stats:kb:*:daily:*')
        for key in kb_stats_keys:
            key_str = key.decode()
            date_str = key_str.split(':')[-1]
            if date_str < cutoff_date:
                self.redis_master.delete(key)
                cleaned_count += 1
        
        logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç»Ÿè®¡ç¼“å­˜")
    
    def collect_cache_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç¼“å­˜æŒ‡æ ‡"""
        try:
            info = self.redis_slave.info()
            
            metrics = {
                'memory_usage': info['used_memory'],
                'memory_usage_human': info['used_memory_human'],
                'memory_peak': info['used_memory_peak'],
                'memory_peak_human': info['used_memory_peak_human'],
                'connected_clients': info['connected_clients'],
                'total_commands_processed': info['total_commands_processed'],
                'instantaneous_ops_per_sec': info['instantaneous_ops_per_sec'],
                'keyspace_hits': info['keyspace_hits'],
                'keyspace_misses': info['keyspace_misses'],
                'evicted_keys': info['evicted_keys'],
                'expired_keys': info['expired_keys'],
            }
            
            # è®¡ç®—å‘½ä¸­ç‡
            total_requests = info['keyspace_hits'] + info['keyspace_misses']
            if total_requests > 0:
                metrics['hit_rate'] = info['keyspace_hits'] / total_requests
            else:
                metrics['hit_rate'] = 0.0
            
            # è‡ªå®šä¹‰æŒ‡æ ‡
            metrics.update({
                'hotspot_keys_count': self.redis_slave.zcard('hotspot:kp:daily'),
                'active_users_count': len(self.redis_slave.keys('user:online:*')),
                'cache_layers_stats': self._get_cache_layers_stats(),
                'timestamp': datetime.now().isoformat()
            })
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†ç¼“å­˜æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def _get_cache_layers_stats(self) -> Dict[str, int]:
        """è·å–å„ç¼“å­˜å±‚ç»Ÿè®¡"""
        stats = {}
        
        try:
            # L1 çƒ­ç‚¹ç¼“å­˜
            l1_keys = self.redis_slave.keys('hotspot:*')
            stats['l1_hotspot'] = len(l1_keys)
            
            # L2 ä¼šè¯ç¼“å­˜
            l2_keys = self.redis_slave.keys('user:info:*')
            stats['l2_session'] = len(l2_keys)
            
            # L3 å†…å®¹ç¼“å­˜
            l3_keys = self.redis_slave.keys('kp:content:*')
            stats['l3_content'] = len(l3_keys)
            
            # L4 ç»Ÿè®¡ç¼“å­˜
            l4_keys = self.redis_slave.keys('stats:*')
            stats['l4_statistics'] = len(l4_keys)
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜å±‚ç»Ÿè®¡å¤±è´¥: {e}")
            stats = {'l1_hotspot': 0, 'l2_session': 0, 'l3_content': 0, 'l4_statistics': 0}
        
        return stats
    
    def verify_cache_consistency(self) -> List[Dict[str, Any]]:
        """éªŒè¯ç¼“å­˜ä¸æ•°æ®åº“çš„ä¸€è‡´æ€§"""
        logger.info("å¼€å§‹ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥...")
        
        inconsistencies = []
        
        try:
            # æ£€æŸ¥çŸ¥è¯†åº“ä¿¡æ¯ä¸€è‡´æ€§
            inconsistencies.extend(self._check_knowledge_base_consistency())
            
            # æ£€æŸ¥ç”¨æˆ·çŠ¶æ€ä¸€è‡´æ€§
            inconsistencies.extend(self._check_user_state_consistency())
            
            if inconsistencies:
                logger.warning(f"å‘ç° {len(inconsistencies)} ä¸ªä¸ä¸€è‡´é—®é¢˜")
            else:
                logger.info("ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡!")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            inconsistencies.append({
                'type': 'check_error',
                'error': str(e)
            })
        
        return inconsistencies
    
    def _check_knowledge_base_consistency(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥çŸ¥è¯†åº“ä¿¡æ¯ä¸€è‡´æ€§"""
        inconsistencies = []
        kb_keys = self.redis_slave.keys('kb:info:*')
        
        for key in kb_keys[:20]:  # æ£€æŸ¥å‰20ä¸ª
            kb_id = key.decode().split(':')[-1]
            cached_data = self.redis_slave.hgetall(key)
            
            # ä»æ•°æ®åº“è·å–æ•°æ®
            cursor = self.db_connection.cursor(dictionary=True)
            cursor.execute("""
                SELECT id, name, owner_id, visibility, content_count, subscriber_count
                FROM knowledge_bases WHERE id = %s
            """, [kb_id])
            
            db_data = cursor.fetchone()
            cursor.close()
            
            if db_data:
                # æ¯”è¾ƒå…³é”®å­—æ®µ
                if (cached_data.get(b'name', b'').decode() != db_data['name'] or
                    cached_data.get(b'subscriber_count', b'').decode() != str(db_data['subscriber_count'])):
                    inconsistencies.append({
                        'type': 'knowledge_base',
                        'id': kb_id,
                        'issue': 'data_mismatch',
                        'cached': {k.decode(): v.decode() for k, v in cached_data.items()},
                        'database': db_data
                    })
        
        return inconsistencies
    
    def _check_user_state_consistency(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ç”¨æˆ·çŠ¶æ€ä¸€è‡´æ€§"""
        inconsistencies = []
        state_keys = self.redis_slave.keys('user:kp:state:*')
        
        # éšæœºæŠ½æ ·æ£€æŸ¥
        sample_keys = random.sample(state_keys, min(50, len(state_keys)))
        
        for key in sample_keys:
            parts = key.decode().split(':')
            if len(parts) >= 5:
                user_id, content_id = parts[3], parts[4]
                cached_data = self.redis_slave.hgetall(key)
                
                # ä»æ•°æ®åº“è·å–æ•°æ®
                cursor = self.db_connection.cursor(dictionary=True)
                cursor.execute("""
                    SELECT mastery_level, review_count, correct_count, 
                           consecutive_correct, last_reviewed, next_review
                    FROM user_knowledge_point_states 
                    WHERE user_id = %s AND content_id = %s
                """, [user_id, content_id])
                
                db_data = cursor.fetchone()
                cursor.close()
                
                if db_data:
                    # æ¯”è¾ƒå…³é”®å­—æ®µ
                    if (cached_data.get(b'mastery_level', b'').decode() != str(db_data['mastery_level']) or
                        cached_data.get(b'review_count', b'').decode() != str(db_data['review_count'])):
                        inconsistencies.append({
                            'type': 'user_state',
                            'user_id': user_id,
                            'content_id': content_id,
                            'issue': 'data_mismatch'
                        })
        
        return inconsistencies
    
    def update_access_counter(self, resource_type: str, resource_id: int):
        """æ›´æ–°è®¿é—®è®¡æ•°å™¨"""
        key = f"access:count:{resource_type}:{resource_id}"
        
        # å¢åŠ è®¡æ•°
        current_count = self.redis_master.incr(key)
        
        # è®¾ç½®è¿‡æœŸæ—¶é—´
        if current_count == 1:
            self.redis_master.expire(key, self.config['cache']['access_counter_ttl'])
        
        # æ›´æ–°çƒ­ç‚¹æ’è¡Œ
        if current_count % self.config['cache']['access_counter_batch_size'] == 0:
            self._update_hotspot_ranking(resource_type, resource_id, current_count)
    
    def _update_hotspot_ranking(self, resource_type: str, resource_id: int, score: int):
        """æ›´æ–°çƒ­ç‚¹æ’è¡Œ"""
        hotspot_key = f"hotspot:{resource_type}:daily"
        
        # æ·»åŠ åˆ°æœ‰åºé›†åˆ
        self.redis_master.zadd(hotspot_key, {str(resource_id): score})
        
        # ä¿æŒæ’è¡Œæ¦œå¤§å° (åªä¿ç•™å‰1000)
        self.redis_master.zremrangebyrank(hotspot_key, 0, -1001)
        
        # è®¾ç½®è¿‡æœŸæ—¶é—´
        self.redis_master.expire(hotspot_key, 86400)  # 24å°æ—¶
    
    def _compress_large_object(self, data: Any) -> str:
        """å‹ç¼©å¤§å‹JSONå¯¹è±¡"""
        json_str = json.dumps(data, ensure_ascii=False)
        
        if len(json_str) > 1024:  # å¤§äº1KBçš„å¯¹è±¡è¿›è¡Œå‹ç¼©
            compressed = gzip.compress(json_str.encode('utf-8'))
            return base64.b64encode(compressed).decode('ascii')
        
        return json_str
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.db_connection:
            self.db_connection.close()
        logger.info("ç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")

def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®"""
    config = {
        'redis': {
            'master': {
                'host': os.getenv('REDIS_MASTER_HOST', 'localhost'),
                'port': int(os.getenv('REDIS_MASTER_PORT', 6379)),
                'password': os.getenv('REDIS_MASTER_PASSWORD')
            },
            'slave': {
                'host': os.getenv('REDIS_SLAVE_HOST', 'localhost'),
                'port': int(os.getenv('REDIS_SLAVE_PORT', 6379)),
                'password': os.getenv('REDIS_SLAVE_PASSWORD')
            }
        },
        'database': {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 3306)),
            'user': os.getenv('DB_USER', 'root'),
            'password': os.getenv('DB_PASSWORD', ''),
            'name': os.getenv('DB_NAME', 'memorin')
        },
        'cache': {
            'warmup_popular_kb_count': 20,
            'warmup_hot_content_count': 50,
            'cleanup_low_access_threshold': 5,
            'cleanup_expired_session_threshold': 1800,
            'access_counter_ttl': 3600,
            'access_counter_batch_size': 10
        }
    }
    
    return config

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Memorin Redisç¼“å­˜ç®¡ç†å·¥å…·')
    parser.add_argument('--warmup', action='store_true', help='æ‰§è¡Œç¼“å­˜é¢„çƒ­')
    parser.add_argument('--cleanup', action='store_true', help='æ‰§è¡Œç¼“å­˜æ¸…ç†')
    parser.add_argument('--monitor', action='store_true', help='æ˜¾ç¤ºç›‘æ§æŒ‡æ ‡')
    parser.add_argument('--check', action='store_true', help='æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache_manager = RedisCacheManager(config)
    
    try:
        if args.warmup:
            cache_manager.warmup_cache()
        
        elif args.cleanup:
            cache_manager.cleanup_expired_cache()
        
        elif args.monitor:
            metrics = cache_manager.collect_cache_metrics()
            print(json.dumps(metrics, indent=2, ensure_ascii=False))
        
        elif args.check:
            inconsistencies = cache_manager.verify_cache_consistency()
            if inconsistencies:
                print(json.dumps(inconsistencies, indent=2, ensure_ascii=False))
            else:
                print("ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        
        else:
            parser.print_help()
    
    finally:
        cache_manager.close()

if __name__ == '__main__':
    main() 