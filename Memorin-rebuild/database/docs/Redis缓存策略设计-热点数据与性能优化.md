# Memorin Redisç¼“å­˜ç­–ç•¥è®¾è®¡ - çƒ­ç‚¹æ•°æ®ä¸æ€§èƒ½ä¼˜åŒ–

> **ç‰ˆæœ¬**: v2.0  
> **åˆ›å»ºæ—¶é—´**: 2025-01-08  
> **æ¶æ„**: åˆ†å±‚ç¼“å­˜ã€è¯»å†™åˆ†ç¦»ã€çƒ­ç‚¹è¯†åˆ«  

## ğŸ“‹ ç¼“å­˜ç­–ç•¥æ¦‚è¿°

åŸºäºMemorinæ™ºèƒ½çŸ¥è¯†å¤ä¹ ç³»ç»Ÿçš„ä¸šåŠ¡ç‰¹ç‚¹ï¼Œè®¾è®¡äº†å¤šå±‚æ¬¡ã€é«˜æ€§èƒ½çš„Redisç¼“å­˜æ¶æ„ï¼Œé€šè¿‡çƒ­ç‚¹æ•°æ®è¯†åˆ«ã€æ™ºèƒ½è¿‡æœŸç­–ç•¥å’Œè¯»å†™ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿå“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒã€‚

### è®¾è®¡ç›®æ ‡

1. **æ€§èƒ½æå‡**: å°†æ•°æ®åº“æŸ¥è¯¢å“åº”æ—¶é—´ä»100msé™ä½åˆ°5msä»¥å†…
2. **å¹¶å‘æ”¯æŒ**: æ”¯æŒ1000+å¹¶å‘ç”¨æˆ·åŒæ—¶åœ¨çº¿å­¦ä¹ 
3. **æ•°æ®ä¸€è‡´æ€§**: ä¿è¯ç¼“å­˜ä¸æ•°æ®åº“æ•°æ®çš„æœ€ç»ˆä¸€è‡´æ€§
4. **æ™ºèƒ½ç¼“å­˜**: åŸºäºç”¨æˆ·è¡Œä¸ºçš„æ™ºèƒ½çƒ­ç‚¹æ•°æ®é¢„åŠ è½½
5. **æˆæœ¬ä¼˜åŒ–**: åˆç†çš„å†…å­˜ä½¿ç”¨å’Œè¿‡æœŸç­–ç•¥æ§åˆ¶æˆæœ¬

## ğŸ—ï¸ ç¼“å­˜æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```mermaid
graph TB
    A[å®¢æˆ·ç«¯åº”ç”¨] --> B[APIç½‘å…³]
    B --> C[åº”ç”¨æœåŠ¡å±‚]
    C --> D[Redisç¼“å­˜å±‚]
    C --> E[MySQLæ•°æ®åº“]
    
    D --> D1[L1: çƒ­ç‚¹æ•°æ®ç¼“å­˜<br/>TTL: 1å°æ—¶]
    D --> D2[L2: ç”¨æˆ·ä¼šè¯ç¼“å­˜<br/>TTL: 24å°æ—¶]
    D --> D3[L3: çŸ¥è¯†åº“å†…å®¹ç¼“å­˜<br/>TTL: 7å¤©]
    D --> D4[L4: ç»Ÿè®¡èšåˆç¼“å­˜<br/>TTL: 5åˆ†é’Ÿ]
    
    F[ç¼“å­˜ç®¡ç†å™¨] --> D
    F --> G[çƒ­ç‚¹æ•°æ®è¯†åˆ«]
    F --> H[ç¼“å­˜é¢„çƒ­]
    F --> I[ç¼“å­˜æ¸…ç†]
    
    J[ç›‘æ§ç³»ç»Ÿ] --> D
    J --> K[æ€§èƒ½æŒ‡æ ‡]
    J --> L[å‘½ä¸­ç‡åˆ†æ]
```

### ç¼“å­˜åˆ†å±‚ç­–ç•¥

| ç¼“å­˜å±‚çº§ | æ•°æ®ç±»å‹ | TTL | æ›´æ–°ç­–ç•¥ | å†…å­˜å ç”¨ |
|----------|----------|-----|----------|----------|
| L1 çƒ­ç‚¹ç¼“å­˜ | é«˜é¢‘è®¿é—®çš„çŸ¥è¯†ç‚¹ã€ç”¨æˆ·å¤ä¹ é˜Ÿåˆ— | 1å°æ—¶ | ä¸»åŠ¨æ›´æ–° | 30% |
| L2 ä¼šè¯ç¼“å­˜ | ç”¨æˆ·è®¤è¯ä¿¡æ¯ã€ä¸ªäººè®¾ç½® | 24å°æ—¶ | è¢«åŠ¨å¤±æ•ˆ | 25% |
| L3 å†…å®¹ç¼“å­˜ | çŸ¥è¯†åº“ã€çŸ¥è¯†åŒºã€çŸ¥è¯†ç‚¹å†…å®¹ | 7å¤© | ç‰ˆæœ¬æ§åˆ¶ | 35% |
| L4 ç»Ÿè®¡ç¼“å­˜ | èšåˆç»Ÿè®¡ã€æ’è¡Œæ¦œ | 5åˆ†é’Ÿ | å®šæ—¶æ›´æ–° | 10% |

## ğŸ—„ï¸ æ•°æ®ç»“æ„è®¾è®¡

### 1. ç”¨æˆ·ä¼šè¯ç¼“å­˜

```redis
# ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
user:info:{user_id}
HASH {
    "id": "123",
    "username": "demo_user",
    "email": "user@example.com",
    "status": "active",
    "last_login": "2025-01-08T10:30:00Z"
}
TTL: 24å°æ—¶

# ç”¨æˆ·è®¾ç½®
user:settings:{user_id}
HASH {
    "theme": "dark",
    "daily_goal": "20",
    "difficulty_preference": "3",
    "notification_enabled": "true"
}
TTL: 24å°æ—¶

# ç”¨æˆ·è®¤è¯ä»¤ç‰Œ
user:token:{token_value}
STRING {user_id}
TTL: æ ¹æ®tokenç±»å‹è®¾ç½® (access: 1å°æ—¶, refresh: 7å¤©)

# ç”¨æˆ·åœ¨çº¿çŠ¶æ€
user:online:{user_id}
STRING "last_activity_timestamp"
TTL: 30åˆ†é’Ÿ
```

### 2. çŸ¥è¯†åº“å†…å®¹ç¼“å­˜

```redis
# çŸ¥è¯†åº“åŸºæœ¬ä¿¡æ¯
kb:info:{kb_id}
HASH {
    "id": "1",
    "name": "è½¯ä»¶å·¥ç¨‹åŸºç¡€",
    "owner_id": "5",
    "visibility": "public",
    "content_count": "150",
    "subscriber_count": "2340"
}
TTL: 7å¤©

# çŸ¥è¯†åº“è¯¦ç»†ä¿¡æ¯
kb:detail:{kb_id}
JSON {
    "metadata": {
        "description": "...",
        "icon": "ğŸ› ï¸",
        "difficulty_level": 3.2,
        "tags": ["è½¯ä»¶å·¥ç¨‹", "ç¼–ç¨‹"]
    },
    "areas": [...],
    "statistics": {...}
}
TTL: 7å¤©

# çŸ¥è¯†ç‚¹å†…å®¹
kp:content:{content_id}
HASH {
    "question": "ä»€ä¹ˆæ˜¯è½¯ä»¶å·¥ç¨‹ï¼Ÿ",
    "answer": "è½¯ä»¶å·¥ç¨‹æ˜¯...",
    "type": "fill",
    "difficulty": "3",
    "explanation": "..."
}
TTL: 7å¤©

# é€‰æ‹©é¢˜é€‰é¡¹
kp:choices:{content_id}
JSON [
    {"key": "A", "text": "é€‰é¡¹A", "is_correct": false},
    {"key": "B", "text": "é€‰é¡¹B", "is_correct": true}
]
TTL: 7å¤©
```

### 3. ç”¨æˆ·å­¦ä¹ çŠ¶æ€ç¼“å­˜

```redis
# ç”¨æˆ·çŸ¥è¯†ç‚¹å­¦ä¹ çŠ¶æ€
user:kp:state:{user_id}:{content_id}
HASH {
    "mastery_level": "4",
    "review_count": "8",
    "correct_count": "7",
    "consecutive_correct": "3",
    "last_reviewed": "2025-01-08T10:30:00Z",
    "next_review": "2025-01-15T09:00:00Z",
    "is_bookmarked": "true"
}
TTL: 6å°æ—¶

# ç”¨æˆ·å¤ä¹ é˜Ÿåˆ—
user:review:queue:{user_id}:{date}
LIST [
    "content_id_1:priority_5:due",
    "content_id_2:priority_3:new",
    "content_id_3:priority_4:mistake"
]
TTL: 12å°æ—¶

# ç”¨æˆ·å­¦ä¹ ä¼šè¯
user:session:{user_id}
HASH {
    "current_kb": "1",
    "current_area": "2",
    "session_start": "2025-01-08T10:00:00Z",
    "items_reviewed": "5",
    "items_correct": "4",
    "estimated_remaining": "15"
}
TTL: 2å°æ—¶
```

### 4. çƒ­ç‚¹æ•°æ®ç¼“å­˜

```redis
# çƒ­ç‚¹çŸ¥è¯†ç‚¹ (åŸºäºè®¿é—®é¢‘æ¬¡)
hotspot:kp:daily
ZSET {
    "content_id_1": 2340,  # scoreä¸ºè®¿é—®æ¬¡æ•°
    "content_id_2": 1890,
    "content_id_3": 1567
}
TTL: 24å°æ—¶

# çƒ­ç‚¹çŸ¥è¯†åº“
hotspot:kb:weekly
ZSET {
    "kb_id_1": 15670,
    "kb_id_2": 12340,
    "kb_id_3": 9876
}
TTL: 7å¤©

# å®æ—¶è®¿é—®è®¡æ•°å™¨
access:count:{resource_type}:{resource_id}
STRING "è®¿é—®æ¬¡æ•°"
TTL: 1å°æ—¶

# ç”¨æˆ·è¡Œä¸ºåºåˆ— (ç”¨äºæ¨è)
user:behavior:{user_id}
LIST [
    "view:kb:1:2025-01-08T10:30:00Z",
    "review:kp:123:2025-01-08T10:31:00Z",
    "bookmark:kp:456:2025-01-08T10:32:00Z"
]
TTL: 24å°æ—¶
```

### 5. ç»Ÿè®¡èšåˆç¼“å­˜

```redis
# å…¨å±€ç»Ÿè®¡
stats:global:daily:{date}
HASH {
    "total_reviews": "15670",
    "total_users_active": "1234",
    "total_new_content": "45",
    "avg_accuracy": "0.78"
}
TTL: 24å°æ—¶

# çŸ¥è¯†åº“ç»Ÿè®¡
stats:kb:{kb_id}:daily:{date}
HASH {
    "reviews": "2340",
    "unique_users": "456",
    "avg_score": "0.82",
    "completion_rate": "0.67"
}
TTL: 24å°æ—¶

# ç”¨æˆ·ä¸ªäººç»Ÿè®¡
stats:user:{user_id}:summary
HASH {
    "total_reviews": "1567",
    "accuracy_rate": "0.85",
    "streak_days": "12",
    "mastered_points": "234",
    "daily_goal_progress": "0.8"
}
TTL: 1å°æ—¶
```

## ğŸ”¥ çƒ­ç‚¹æ•°æ®è¯†åˆ«ç­–ç•¥

### 1. è®¿é—®é¢‘æ¬¡ç»Ÿè®¡

```python
# è®¿é—®è®¡æ•°å™¨æ›´æ–°
def update_access_counter(resource_type, resource_id):
    key = f"access:count:{resource_type}:{resource_id}"
    
    # å¢åŠ è®¡æ•°
    current_count = redis.incr(key)
    
    # è®¾ç½®è¿‡æœŸæ—¶é—´
    if current_count == 1:
        redis.expire(key, 3600)  # 1å°æ—¶
    
    # æ›´æ–°çƒ­ç‚¹æ’è¡Œ
    if current_count % 10 == 0:  # æ¯10æ¬¡è®¿é—®æ›´æ–°ä¸€æ¬¡æ’è¡Œ
        update_hotspot_ranking(resource_type, resource_id, current_count)

# çƒ­ç‚¹æ’è¡Œæ›´æ–°
def update_hotspot_ranking(resource_type, resource_id, score):
    hotspot_key = f"hotspot:{resource_type}:daily"
    
    # æ·»åŠ åˆ°æœ‰åºé›†åˆ
    redis.zadd(hotspot_key, {resource_id: score})
    
    # ä¿æŒæ’è¡Œæ¦œå¤§å° (åªä¿ç•™å‰1000)
    redis.zremrangebyrank(hotspot_key, 0, -1001)
    
    # è®¾ç½®è¿‡æœŸæ—¶é—´
    redis.expire(hotspot_key, 86400)  # 24å°æ—¶
```

### 2. æ™ºèƒ½é¢„åŠ è½½

```python
# åŸºäºç”¨æˆ·è¡Œä¸ºé¢„æµ‹çš„é¢„åŠ è½½
def preload_user_data(user_id):
    # è·å–ç”¨æˆ·æœ€è¿‘è¡Œä¸º
    behaviors = redis.lrange(f"user:behavior:{user_id}", 0, 10)
    
    # åˆ†æè¡Œä¸ºæ¨¡å¼
    pattern = analyze_behavior_pattern(behaviors)
    
    # é¢„åŠ è½½ç›¸å…³æ•°æ®
    if pattern['type'] == 'sequential_learning':
        preload_next_knowledge_points(user_id, pattern['current_area'])
    elif pattern['type'] == 'review_session':
        preload_review_queue(user_id)
    elif pattern['type'] == 'exploration':
        preload_recommended_content(user_id)

# åŒºåŸŸçŸ¥è¯†ç‚¹é¢„åŠ è½½
def preload_next_knowledge_points(user_id, area_id):
    # è·å–åŒºåŸŸä¸‹çš„çŸ¥è¯†ç‚¹åˆ—è¡¨
    content_ids = get_area_content_ids(area_id)
    
    # è¿‡æ»¤ç”¨æˆ·å·²å­¦ä¹ çš„å†…å®¹
    learned_ids = get_user_learned_content(user_id, content_ids)
    next_ids = [id for id in content_ids if id not in learned_ids]
    
    # é¢„åŠ è½½å‰5ä¸ªçŸ¥è¯†ç‚¹
    for content_id in next_ids[:5]:
        preload_knowledge_point(content_id)
```

### 3. åŠ¨æ€çƒ­ç‚¹è°ƒæ•´

```python
# åŠ¨æ€çƒ­ç‚¹é˜ˆå€¼è°ƒæ•´
def adjust_hotspot_threshold():
    # è·å–å½“å‰ç³»ç»Ÿè´Ÿè½½
    cpu_usage = get_cpu_usage()
    memory_usage = get_memory_usage()
    
    # æ ¹æ®è´Ÿè½½è°ƒæ•´çƒ­ç‚¹é˜ˆå€¼
    if cpu_usage > 80 or memory_usage > 85:
        # æé«˜çƒ­ç‚¹é˜ˆå€¼ï¼Œå‡å°‘ç¼“å­˜
        current_threshold = redis.get("hotspot:threshold") or 100
        new_threshold = min(current_threshold * 1.2, 500)
        redis.setex("hotspot:threshold", 3600, new_threshold)
    elif cpu_usage < 50 and memory_usage < 60:
        # é™ä½çƒ­ç‚¹é˜ˆå€¼ï¼Œå¢åŠ ç¼“å­˜
        current_threshold = redis.get("hotspot:threshold") or 100
        new_threshold = max(current_threshold * 0.8, 50)
        redis.setex("hotspot:threshold", 3600, new_threshold)
```

## âš¡ ç¼“å­˜æ›´æ–°ç­–ç•¥

### 1. å†™å…¥ç­–ç•¥

#### Cache-Aside æ¨¡å¼ (ä¸»è¦ä½¿ç”¨)
```python
# è¯»å–æ•°æ®
def get_knowledge_point(content_id):
    # å…ˆä»ç¼“å­˜è¯»å–
    cached_data = redis.hgetall(f"kp:content:{content_id}")
    
    if cached_data:
        return cached_data
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“è¯»å–
    data = db.query_knowledge_point(content_id)
    
    if data:
        # å†™å…¥ç¼“å­˜
        redis.hset(f"kp:content:{content_id}", mapping=data)
        redis.expire(f"kp:content:{content_id}", 604800)  # 7å¤©
    
    return data

# æ›´æ–°æ•°æ®
def update_knowledge_point(content_id, updates):
    # å…ˆæ›´æ–°æ•°æ®åº“
    success = db.update_knowledge_point(content_id, updates)
    
    if success:
        # åˆ é™¤ç¼“å­˜ï¼Œä¸‹æ¬¡è¯»å–æ—¶é‡æ–°åŠ è½½
        redis.delete(f"kp:content:{content_id}")
        
        # å¦‚æœæ˜¯çƒ­ç‚¹æ•°æ®ï¼Œç«‹å³é‡æ–°ç¼“å­˜
        if is_hotspot_content(content_id):
            get_knowledge_point(content_id)  # é‡æ–°åŠ è½½åˆ°ç¼“å­˜
    
    return success
```

#### Write-Through æ¨¡å¼ (ç”¨æˆ·çŠ¶æ€æ•°æ®)
```python
# æ›´æ–°ç”¨æˆ·å­¦ä¹ çŠ¶æ€
def update_user_learning_state(user_id, content_id, state_data):
    cache_key = f"user:kp:state:{user_id}:{content_id}"
    
    # åŒæ—¶æ›´æ–°æ•°æ®åº“å’Œç¼“å­˜
    db_success = db.update_user_state(user_id, content_id, state_data)
    
    if db_success:
        redis.hset(cache_key, mapping=state_data)
        redis.expire(cache_key, 21600)  # 6å°æ—¶
        
        # æ›´æ–°ç›¸å…³èšåˆæ•°æ®
        update_user_statistics_cache(user_id)
    
    return db_success
```

### 2. å¤±æ•ˆç­–ç•¥

#### åŸºäºç‰ˆæœ¬çš„å¤±æ•ˆ
```python
# çŸ¥è¯†åº“ç‰ˆæœ¬æ§åˆ¶
def update_knowledge_base(kb_id, updates):
    # è·å–å½“å‰ç‰ˆæœ¬
    current_version = redis.get(f"kb:version:{kb_id}") or "1.0.0"
    new_version = increment_version(current_version)
    
    # æ›´æ–°æ•°æ®åº“
    db.update_knowledge_base(kb_id, updates, new_version)
    
    # æ›´æ–°ç‰ˆæœ¬å·
    redis.set(f"kb:version:{kb_id}", new_version)
    
    # å¤±æ•ˆç›¸å…³ç¼“å­˜
    invalidate_kb_cache(kb_id)

def get_knowledge_base(kb_id):
    cached_version = redis.hget(f"kb:info:{kb_id}", "version")
    current_version = redis.get(f"kb:version:{kb_id}")
    
    # ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œéœ€è¦é‡æ–°åŠ è½½
    if cached_version != current_version:
        reload_knowledge_base_cache(kb_id)
    
    return redis.hgetall(f"kb:info:{kb_id}")
```

#### åŸºäºäº‹ä»¶çš„å¤±æ•ˆ
```python
# äº‹ä»¶é©±åŠ¨çš„ç¼“å­˜å¤±æ•ˆ
def handle_content_update_event(event_data):
    content_id = event_data['content_id']
    kb_id = event_data['knowledge_base_id']
    area_id = event_data['knowledge_area_id']
    
    # å¤±æ•ˆå†…å®¹ç¼“å­˜
    redis.delete(f"kp:content:{content_id}")
    redis.delete(f"kp:choices:{content_id}")
    
    # å¤±æ•ˆå…³è”çš„çŸ¥è¯†åº“ç¼“å­˜
    redis.delete(f"kb:detail:{kb_id}")
    
    # å¤±æ•ˆç»Ÿè®¡ç¼“å­˜
    pattern = f"stats:kb:{kb_id}:*"
    keys = redis.keys(pattern)
    if keys:
        redis.delete(*keys)
    
    # é‡æ–°è®¡ç®—çƒ­ç‚¹æ’è¡Œ
    recalculate_hotspot_ranking()
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. è¿æ¥æ± ä¼˜åŒ–

```python
# Redisè¿æ¥æ± é…ç½®
import redis
from redis.connection import ConnectionPool

# ä¸»Rediså®ä¾‹ (è¯»å†™)
REDIS_MASTER_POOL = ConnectionPool(
    host='redis-master',
    port=6379,
    password='your_password',
    db=0,
    max_connections=100,
    retry_on_timeout=True,
    socket_timeout=5,
    socket_connect_timeout=5,
    health_check_interval=30
)

# Redisä»å®ä¾‹ (åªè¯»)
REDIS_SLAVE_POOL = ConnectionPool(
    host='redis-slave',
    port=6379,
    password='your_password',
    db=0,
    max_connections=50,
    retry_on_timeout=True,
    socket_timeout=5,
    socket_connect_timeout=5,
    health_check_interval=30
)

redis_master = redis.Redis(connection_pool=REDIS_MASTER_POOL)
redis_slave = redis.Redis(connection_pool=REDIS_SLAVE_POOL)
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
# æ‰¹é‡åŠ è½½ç”¨æˆ·å¤ä¹ é˜Ÿåˆ—
def load_user_review_queue_batch(user_ids, date):
    pipeline = redis_slave.pipeline()
    
    # æ‰¹é‡æŸ¥è¯¢
    for user_id in user_ids:
        pipeline.lrange(f"user:review:queue:{user_id}:{date}", 0, -1)
    
    results = pipeline.execute()
    
    # ç»„è£…ç»“æœ
    queue_data = {}
    for i, user_id in enumerate(user_ids):
        queue_data[user_id] = results[i]
    
    return queue_data

# æ‰¹é‡æ›´æ–°è®¿é—®è®¡æ•°
def batch_update_access_counters(access_records):
    pipeline = redis_master.pipeline()
    
    for record in access_records:
        key = f"access:count:{record['type']}:{record['id']}"
        pipeline.incr(key)
        pipeline.expire(key, 3600)
    
    pipeline.execute()
```

### 3. å†…å­˜ä¼˜åŒ–

```python
# å‹ç¼©å¤§å¯¹è±¡
import json
import gzip
import base64

def compress_large_object(data):
    """å‹ç¼©å¤§å‹JSONå¯¹è±¡"""
    json_str = json.dumps(data, ensure_ascii=False)
    
    if len(json_str) > 1024:  # å¤§äº1KBçš„å¯¹è±¡è¿›è¡Œå‹ç¼©
        compressed = gzip.compress(json_str.encode('utf-8'))
        return base64.b64encode(compressed).decode('ascii')
    
    return json_str

def decompress_large_object(compressed_data):
    """è§£å‹ç¼©JSONå¯¹è±¡"""
    try:
        # å°è¯•è§£å‹ç¼©
        decoded = base64.b64decode(compressed_data.encode('ascii'))
        decompressed = gzip.decompress(decoded)
        return json.loads(decompressed.decode('utf-8'))
    except:
        # å¦‚æœè§£å‹å¤±è´¥ï¼Œè¯´æ˜æ˜¯æœªå‹ç¼©çš„æ•°æ®
        return json.loads(compressed_data)

# ä½¿ç”¨ç¤ºä¾‹
def cache_knowledge_base_detail(kb_id, detail_data):
    compressed_data = compress_large_object(detail_data)
    redis.setex(f"kb:detail:{kb_id}", 604800, compressed_data)

def get_knowledge_base_detail(kb_id):
    compressed_data = redis.get(f"kb:detail:{kb_id}")
    if compressed_data:
        return decompress_large_object(compressed_data)
    return None
```

### 4. åˆ†ç‰‡ç­–ç•¥

```python
# åŸºäºç”¨æˆ·IDçš„åˆ†ç‰‡
def get_user_shard(user_id):
    """æ ¹æ®ç”¨æˆ·IDç¡®å®šåˆ†ç‰‡"""
    return user_id % 4  # 4ä¸ªåˆ†ç‰‡

def get_redis_connection(shard_id):
    """è·å–æŒ‡å®šåˆ†ç‰‡çš„Redisè¿æ¥"""
    shard_configs = {
        0: {'host': 'redis-shard-0', 'port': 6379},
        1: {'host': 'redis-shard-1', 'port': 6379},
        2: {'host': 'redis-shard-2', 'port': 6379},
        3: {'host': 'redis-shard-3', 'port': 6379},
    }
    
    config = shard_configs[shard_id]
    return redis.Redis(host=config['host'], port=config['port'])

# åˆ†ç‰‡æ“ä½œç¤ºä¾‹
def set_user_data(user_id, data):
    shard_id = get_user_shard(user_id)
    redis_conn = get_redis_connection(shard_id)
    redis_conn.hset(f"user:info:{user_id}", mapping=data)

def get_user_data(user_id):
    shard_id = get_user_shard(user_id)
    redis_conn = get_redis_connection(shard_id)
    return redis_conn.hgetall(f"user:info:{user_id}")
```

## ğŸ“ˆ ç›‘æ§å’ŒæŒ‡æ ‡

### 1. å…³é”®æ€§èƒ½æŒ‡æ ‡

```python
# ç¼“å­˜æ€§èƒ½ç›‘æ§
class CacheMonitor:
    def __init__(self):
        self.hit_count = 0
        self.miss_count = 0
        self.total_requests = 0
    
    def record_hit(self, cache_type):
        self.hit_count += 1
        self.total_requests += 1
        
        # è®°å½•åˆ°Redisç”¨äºç›‘æ§
        redis.incr(f"metrics:cache:hit:{cache_type}")
        redis.incr("metrics:cache:hit:total")
    
    def record_miss(self, cache_type):
        self.miss_count += 1
        self.total_requests += 1
        
        redis.incr(f"metrics:cache:miss:{cache_type}")
        redis.incr("metrics:cache:miss:total")
    
    def get_hit_rate(self):
        if self.total_requests == 0:
            return 0
        return self.hit_count / self.total_requests
    
    def get_daily_stats(self, date):
        """è·å–æ—¥ç»Ÿè®¡"""
        hit_total = redis.get(f"metrics:cache:hit:total:{date}") or 0
        miss_total = redis.get(f"metrics:cache:miss:total:{date}") or 0
        
        total = int(hit_total) + int(miss_total)
        hit_rate = int(hit_total) / total if total > 0 else 0
        
        return {
            'hit_count': int(hit_total),
            'miss_count': int(miss_total),
            'hit_rate': hit_rate,
            'total_requests': total
        }

# ä½¿ç”¨ç¤ºä¾‹
monitor = CacheMonitor()

def cached_get_knowledge_point(content_id):
    data = redis.hgetall(f"kp:content:{content_id}")
    
    if data:
        monitor.record_hit('knowledge_point')
        return data
    else:
        monitor.record_miss('knowledge_point')
        # ä»æ•°æ®åº“åŠ è½½...
        return load_from_database(content_id)
```

### 2. å®æ—¶ç›‘æ§ä»ªè¡¨æ¿

```python
# ç›‘æ§æŒ‡æ ‡æ”¶é›†
def collect_cache_metrics():
    """æ”¶é›†ç¼“å­˜æŒ‡æ ‡"""
    info = redis.info()
    
    metrics = {
        'memory_usage': info['used_memory'],
        'memory_usage_human': info['used_memory_human'],
        'connected_clients': info['connected_clients'],
        'total_commands_processed': info['total_commands_processed'],
        'keyspace_hits': info['keyspace_hits'],
        'keyspace_misses': info['keyspace_misses'],
        'hit_rate': info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses']) if (info['keyspace_hits'] + info['keyspace_misses']) > 0 else 0
    }
    
    # è‡ªå®šä¹‰æŒ‡æ ‡
    metrics.update({
        'hotspot_keys_count': redis.zcard('hotspot:kp:daily'),
        'active_users_count': len(redis.keys('user:online:*')),
        'cache_layers_stats': get_cache_layers_stats()
    })
    
    return metrics

def get_cache_layers_stats():
    """è·å–å„ç¼“å­˜å±‚ç»Ÿè®¡"""
    stats = {}
    
    # L1 çƒ­ç‚¹ç¼“å­˜
    l1_keys = redis.keys('hotspot:*')
    stats['l1_hotspot'] = len(l1_keys)
    
    # L2 ä¼šè¯ç¼“å­˜
    l2_keys = redis.keys('user:info:*')
    stats['l2_session'] = len(l2_keys)
    
    # L3 å†…å®¹ç¼“å­˜
    l3_keys = redis.keys('kp:content:*')
    stats['l3_content'] = len(l3_keys)
    
    # L4 ç»Ÿè®¡ç¼“å­˜
    l4_keys = redis.keys('stats:*')
    stats['l4_statistics'] = len(l4_keys)
    
    return stats
```

## ğŸ”§ ç¼“å­˜ç®¡ç†å·¥å…·

### 1. ç¼“å­˜é¢„çƒ­è„šæœ¬

```python
# ç³»ç»Ÿå¯åŠ¨æ—¶çš„ç¼“å­˜é¢„çƒ­
def warmup_cache():
    """ç³»ç»Ÿå¯åŠ¨æ—¶é¢„çƒ­å…³é”®ç¼“å­˜"""
    print("å¼€å§‹ç¼“å­˜é¢„çƒ­...")
    
    # 1. é¢„çƒ­çƒ­ç‚¹çŸ¥è¯†åº“
    popular_kbs = db.query("""
        SELECT id FROM knowledge_bases 
        WHERE visibility = 'public' 
        ORDER BY subscriber_count DESC 
        LIMIT 20
    """)
    
    for kb in popular_kbs:
        preload_knowledge_base(kb['id'])
    
    # 2. é¢„çƒ­ç³»ç»Ÿé…ç½®
    preload_system_configs()
    
    # 3. é¢„çƒ­ç»Ÿè®¡æ•°æ®
    preload_global_statistics()
    
    print("ç¼“å­˜é¢„çƒ­å®Œæˆ!")

def preload_knowledge_base(kb_id):
    """é¢„åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
    # åŠ è½½åŸºæœ¬ä¿¡æ¯
    kb_info = db.get_knowledge_base_info(kb_id)
    redis.hset(f"kb:info:{kb_id}", mapping=kb_info)
    redis.expire(f"kb:info:{kb_id}", 604800)
    
    # åŠ è½½è¯¦ç»†ä¿¡æ¯
    kb_detail = db.get_knowledge_base_detail(kb_id)
    compressed_detail = compress_large_object(kb_detail)
    redis.setex(f"kb:detail:{kb_id}", 604800, compressed_detail)
    
    # åŠ è½½çƒ­ç‚¹çŸ¥è¯†ç‚¹
    hot_contents = db.query("""
        SELECT content_id FROM knowledge_base_content_relations 
        WHERE knowledge_base_id = %s 
        ORDER BY usage_count DESC 
        LIMIT 50
    """, [kb_id])
    
    for content in hot_contents:
        preload_knowledge_point(content['content_id'])

def preload_knowledge_point(content_id):
    """é¢„åŠ è½½çŸ¥è¯†ç‚¹å†…å®¹"""
    content_data = db.get_knowledge_point_content(content_id)
    redis.hset(f"kp:content:{content_id}", mapping=content_data)
    redis.expire(f"kp:content:{content_id}", 604800)
    
    # å¦‚æœæ˜¯é€‰æ‹©é¢˜ï¼ŒåŠ è½½é€‰é¡¹
    if content_data.get('type') == 'choice':
        choices = db.get_knowledge_point_choices(content_id)
        redis.setex(f"kp:choices:{content_id}", 604800, json.dumps(choices))
```

### 2. ç¼“å­˜æ¸…ç†è„šæœ¬

```python
# å®šæœŸç¼“å­˜æ¸…ç†
def cleanup_expired_cache():
    """æ¸…ç†è¿‡æœŸå’Œä½ä»·å€¼ç¼“å­˜"""
    print("å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜...")
    
    # 1. æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·ä¼šè¯
    cleanup_expired_user_sessions()
    
    # 2. æ¸…ç†ä½è®¿é—®é‡çš„å†…å®¹ç¼“å­˜
    cleanup_low_value_content_cache()
    
    # 3. æ¸…ç†è¿‡æœŸçš„ç»Ÿè®¡ç¼“å­˜
    cleanup_expired_statistics()
    
    # 4. å†…å­˜æ•´ç†
    redis.execute_command('MEMORY', 'PURGE')
    
    print("ç¼“å­˜æ¸…ç†å®Œæˆ!")

def cleanup_expired_user_sessions():
    """æ¸…ç†è¿‡æœŸç”¨æˆ·ä¼šè¯"""
    # è·å–æ‰€æœ‰åœ¨çº¿ç”¨æˆ·é”®
    online_keys = redis.keys('user:online:*')
    
    for key in online_keys:
        last_activity = redis.get(key)
        if last_activity:
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡30åˆ†é’Ÿæœªæ´»åŠ¨
            if time.time() - float(last_activity) > 1800:
                user_id = key.split(':')[-1]
                # æ¸…ç†ç›¸å…³ç¼“å­˜
                redis.delete(f"user:online:{user_id}")
                redis.delete(f"user:session:{user_id}")

def cleanup_low_value_content_cache():
    """æ¸…ç†ä½ä»·å€¼å†…å®¹ç¼“å­˜"""
    # è·å–è®¿é—®è®¡æ•°å™¨
    access_keys = redis.keys('access:count:kp:*')
    
    for key in access_keys:
        count = redis.get(key)
        if count and int(count) < 5:  # è®¿é—®æ¬¡æ•°å°‘äº5æ¬¡
            content_id = key.split(':')[-1]
            # åˆ é™¤å†…å®¹ç¼“å­˜
            redis.delete(f"kp:content:{content_id}")
            redis.delete(f"kp:choices:{content_id}")
```

### 3. ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥

```python
# ç¼“å­˜ä¸€è‡´æ€§éªŒè¯
def verify_cache_consistency():
    """éªŒè¯ç¼“å­˜ä¸æ•°æ®åº“çš„ä¸€è‡´æ€§"""
    print("å¼€å§‹ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥...")
    
    inconsistencies = []
    
    # æ£€æŸ¥çŸ¥è¯†åº“ä¿¡æ¯ä¸€è‡´æ€§
    kb_keys = redis.keys('kb:info:*')
    for key in kb_keys:
        kb_id = key.split(':')[-1]
        cached_data = redis.hgetall(key)
        db_data = db.get_knowledge_base_info(kb_id)
        
        if not compare_data(cached_data, db_data):
            inconsistencies.append({
                'type': 'knowledge_base',
                'id': kb_id,
                'issue': 'data_mismatch'
            })
    
    # æ£€æŸ¥ç”¨æˆ·çŠ¶æ€ä¸€è‡´æ€§
    state_keys = redis.keys('user:kp:state:*')
    sample_keys = random.sample(state_keys, min(100, len(state_keys)))
    
    for key in sample_keys:
        parts = key.split(':')
        user_id, content_id = parts[3], parts[4]
        
        cached_data = redis.hgetall(key)
        db_data = db.get_user_knowledge_point_state(user_id, content_id)
        
        if not compare_data(cached_data, db_data):
            inconsistencies.append({
                'type': 'user_state',
                'user_id': user_id,
                'content_id': content_id,
                'issue': 'data_mismatch'
            })
    
    if inconsistencies:
        print(f"å‘ç° {len(inconsistencies)} ä¸ªä¸ä¸€è‡´é—®é¢˜")
        return inconsistencies
    else:
        print("ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡!")
        return []

def compare_data(cached_data, db_data):
    """æ¯”è¾ƒç¼“å­˜æ•°æ®å’Œæ•°æ®åº“æ•°æ®"""
    # å¿½ç•¥æ—¶é—´æˆ³å­—æ®µçš„å¾®å°å·®å¼‚
    ignore_fields = ['updated_at', 'last_checked']
    
    for field in ignore_fields:
        cached_data.pop(field, None)
        db_data.pop(field, None)
    
    return cached_data == db_data
```

## ğŸ“‹ éƒ¨ç½²å’Œé…ç½®

### 1. Redis é…ç½®ä¼˜åŒ–

```conf
# redis.conf ä¼˜åŒ–é…ç½®

# å†…å­˜è®¾ç½®
maxmemory 8gb
maxmemory-policy allkeys-lru

# æŒä¹…åŒ–è®¾ç½®
save 900 1
save 300 10
save 60 10000

# ç½‘ç»œè®¾ç½®
tcp-keepalive 60
timeout 0

# å®¢æˆ·ç«¯è®¾ç½®
maxclients 10000

# æ…¢æŸ¥è¯¢è®¾ç½®
slowlog-log-slower-than 10000
slowlog-max-len 128

# é”®ç©ºé—´é€šçŸ¥
notify-keyspace-events Ex

# AOFæŒä¹…åŒ–
appendonly yes
appendfsync everysec

# å†…å­˜ä¼˜åŒ–
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
```

### 2. ç›‘æ§å‘Šè­¦é…ç½®

```yaml
# prometheus ç›‘æ§é…ç½®
alerts:
  - name: Redisé«˜å†…å­˜ä½¿ç”¨ç‡
    condition: redis_memory_usage_bytes / redis_maxmemory_bytes > 0.85
    action: è§¦å‘ç¼“å­˜æ¸…ç†

  - name: Redisè¿æ¥æ•°è¿‡é«˜
    condition: redis_connected_clients > 8000
    action: æ‰©å®¹Rediså®ä¾‹

  - name: ç¼“å­˜å‘½ä¸­ç‡ä½
    condition: redis_keyspace_hits / (redis_keyspace_hits + redis_keyspace_misses) < 0.8
    action: æ£€æŸ¥ç¼“å­˜ç­–ç•¥

  - name: çƒ­ç‚¹æ•°æ®å¤±è¡¡
    condition: hotspot_keys_count > 10000
    action: è°ƒæ•´çƒ­ç‚¹é˜ˆå€¼
```

## ğŸ¯ æ€§èƒ½ç›®æ ‡å’Œé¢„æœŸæ•ˆæœ

### ç›®æ ‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ | æ”¹å–„å¹…åº¦ |
|------|--------|--------|----------|
| ç¼“å­˜å‘½ä¸­ç‡ | > 90% | 85% | +5% |
| å¹³å‡å“åº”æ—¶é—´ | < 5ms | 100ms | -95% |
| å¹¶å‘ç”¨æˆ·æ•° | 1000+ | 200 | +400% |
| å†…å­˜ä½¿ç”¨æ•ˆç‡ | > 80% | 60% | +20% |
| æ•°æ®åº“æŸ¥è¯¢å‡å°‘ | 80% | - | æ–°å¢æŒ‡æ ‡ |

### é¢„æœŸæ•ˆæœ

1. **ç”¨æˆ·ä½“éªŒæå‡**
   - é¡µé¢åŠ è½½é€Ÿåº¦æå‡95%
   - å¤ä¹ æ“ä½œå“åº”æ—¶é—´ç¼©çŸ­90%
   - æ”¯æŒæ›´å¤šå¹¶å‘ç”¨æˆ·å­¦ä¹ 

2. **ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–**
   - æ•°æ®åº“è´Ÿè½½é™ä½80%
   - æœåŠ¡å™¨èµ„æºåˆ©ç”¨ç‡æå‡
   - ç³»ç»Ÿç¨³å®šæ€§å¢å¼º

3. **è¿ç»´æˆæœ¬é™ä½**
   - å‡å°‘æ•°æ®åº“æœåŠ¡å™¨å‹åŠ›
   - é™ä½ç½‘ç»œå¸¦å®½ä½¿ç”¨
   - æé«˜ç³»ç»Ÿå¯æ‰©å±•æ€§

---

**é‡è¦æé†’**: 
- ç¼“å­˜ç­–ç•¥éœ€è¦æ ¹æ®å®é™…ä¸šåŠ¡è´Ÿè½½è°ƒæ•´
- å®šæœŸç›‘æ§ç¼“å­˜æ€§èƒ½å’Œå‘½ä¸­ç‡
- å»ºç«‹å®Œå–„çš„ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°æœºåˆ¶
- åšå¥½ç¼“å­˜å®¹é‡è§„åˆ’å’Œæ‰©å®¹å‡†å¤‡ 